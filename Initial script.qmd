---
title: "Untitled"
format: html
editor: visual
---

## Preliminaries: packages and data loading

### Packages

```{r include=FALSE}
library(readr)
library(tidyverse)
library(dplyr)
library(stringr)
library(RSelenium)
library(openalexR)
library(essurvey)
library(purrr)
library(progress)
library(furrr)
library(parallel)
library(future.apply)
library(httr)
library(jsonlite)
```

### Data

```{r include=FALSE}
altmetrics_data <- read_csv("altmetrics_data.csv")
```

Top 1%:

```{r}
top_1_percent_n <- ceiling(0.01 * nrow(altmetrics_data))

altmetrics_data_p <- altmetrics_data %>%
  arrange(desc(`Facebook mentions`)) %>%
  slice_head(n = top_1_percent_n)

altmetrics_data_p

```

Country affiliation

```{r}
altmetrics_data <- altmetrics_data %>%
  # Step 1: First author's affiliation
  mutate(
    First_Affiliation = str_split(`Affiliations (GRID)`, ";") %>% 
      sapply(function(x) x[1]) 
  ) %>%
  # Step 2: Extract country from the first author's affiliation
  mutate(
    Country = str_extract(First_Affiliation, "\\(([^)]+)\\)|,\\s*(\\b[A-Za-z ]+\\b)$") %>%
      str_remove_all("[\\(\\)]") 
  )


altmetrics_data %>%
  select(First_Affiliation, Country) %>%
 head(100)
```

```{r}

# Define the function to query ROR API
get_country_from_ror <- function(affiliation) {
  # Construct the API URL for the ROR API
  base_url <- "https://api.ror.org/organizations?query="
  response <- GET(URLencode(paste0(base_url, affiliation)))

  if (response$status_code == 200) {
    content_json <- content(response, "text", encoding = "UTF-8")
    results <- tryCatch(fromJSON(content_json)$items, error = function(e) NULL)
    
    if (!is.null(results) && length(results) > 0) {
      country_name <- tryCatch(results[[1]]$country$country_name, error = function(e) NA)
      return(country_name)
    }
  }
  
  return(NA)
}

altmetrics_data <- altmetrics_data %>%
  mutate(
    country = str_extract(
First_Affiliation, ",\\s*([A-Za-z ]+)$") %>%
      str_remove_all("[\\(\\)]"),
    
   
    country = ifelse(is.na(country), sapply(
First_Affiliation, get_country_from_ror), country)
  )

```

21st Century:

```{r}
altmetrics_data$`Publication Date` <- as.Date(altmetrics_data$`Publication Date`, format = "%Y-%m-%d")
```

```{r}
# Function to convert DOIs to OpenAlex format
convert_doi_to_openalex <- function(doi) {
  return(paste0("https://doi.org/", doi))
}

# Apply the function to each DOI in sample_data
altmetrics_data$DOI <- sapply(altmetrics_data$DOI, convert_doi_to_openalex)
```

```{r}
last_century_data <- altmetrics_data %>%
  filter(`Publication Date` >= as.Date("2000-01-01"))
```

Random small sample of data:

```{r}
set.seed(1)
sample_data <- altmetrics_data %>% sample_n(10)

```

```{r}

# Function to fetch data from OpenAlex API without retries or delays
fetch_openalex_data <- function(doi, email = "a.watt2@lse.ac.uk") {
  base_url <- "https://api.openalex.org/works"
  
  url <- paste0(base_url, "/", doi)
  res <- GET(url, add_headers("User-Agent" = "R/OpenAlexExample", "mailto" = email))
  
  if (status_code(res) == 200) {
    return(content(res, "parsed", type = "application/json"))
  } else {
    return(NULL)  # Return NULL if the request fails
  }
}

# Function to reconstruct the abstract from the inverted index
reconstruct_abstract <- function(inverted_index) {
  if (is.null(inverted_index)) return(NA)
  
  term_positions <- unlist(inverted_index, use.names = TRUE)
  terms <- names(term_positions)
  positions <- as.numeric(term_positions)
  
  # Clean up terms by removing trailing single-digit numbers
  clean_terms <- gsub("(\\w+)[0-9]$", "\\1", terms)
  
  # Reconstruct abstract by sorting terms by their positions
  sorted_terms <- clean_terms[order(positions)]
  abstract <- paste(sorted_terms, collapse = " ")
  
  return(abstract)
}

# Function to extract relevant fields from OpenAlex data
extract_openalex_data <- function(x) {
  if (!is.list(x)) return(NULL)
  
  data <- data.frame(
    abstract = reconstruct_abstract(x$abstract_inverted_index),
    so = ifelse(!is.null(x$primary_location$source$display_name), 
                x$primary_location$source$display_name, NA),
    host_organization = ifelse(!is.null(x$primary_location$source$host_organization_name), 
                               x$primary_location$source$host_organization_name, NA),
    type = ifelse(!is.null(x$type), x$type, NA),
    is_retracted = ifelse(!is.null(x$is_retracted), x$is_retracted, NA),
    any_repository_has_fulltext = ifelse(!is.null(x$open_access$any_repository_has_fulltext), 
                                         x$open_access$any_repository_has_fulltext, NA),
    language = ifelse(!is.null(x$language), x$language, NA),
    publication_year = ifelse(!is.null(x$publication_year), x$publication_year, NA),
    is_oa = ifelse(!is.null(x$primary_location$is_oa), x$primary_location$is_oa, NA),
    type_crossref = ifelse(!is.null(x$type_crossref), x$type_crossref, NA),
    countries_distinct_count = ifelse(!is.null(x$countries_distinct_count), x$countries_distinct_count, NA),
    institutions_distinct_count = ifelse(!is.null(x$institutions_distinct_count), x$institutions_distinct_count, NA),
    has_fulltext = ifelse(!is.null(x$has_fulltext), x$has_fulltext, NA),
    cited_by_count = ifelse(!is.null(x$cited_by_count), x$cited_by_count, NA),
    primary_topic_display_name = ifelse(!is.null(x$primary_topic$display_name), 
                                       x$primary_topic$display_name, NA),
    referenced_works_count = ifelse(!is.null(x$referenced_works_count), x$referenced_works_count, NA),
    institution_country_code = ifelse(!is.null(x$authorships),
                                      paste(unique(unlist(lapply(x$authorships, function(author) {
                                        if (!is.null(author$institutions)) {
                                          lapply(author$institutions, function(inst) inst$country_code)
                                        }
                                      }))), collapse = "; "), NA),
    stringsAsFactors = FALSE
  )
  
  return(data)
}

# Progress bar wrapper
fetch_openalex_data_with_progress <- function(dois, email) {
  pb <- progress_bar$new(
    total = length(dois),
    format = "  Fetching [:bar] :percent (:current/:total) ETA: :eta",
    clear = FALSE, width = 60
  )
  
  # Fetch data in parallel
  fetched_data <- future_map(dois, function(doi) {
    pb$tick()
    fetch_openalex_data(doi, email = email)
  }, .options = furrr_options(seed = TRUE, packages = "httr"))
  
  # Remove NULL entries (failed requests)
  fetched_data <- fetched_data[!sapply(fetched_data, is.null)]
  
  return(fetched_data)
}

# Function to save intermediate results
save_intermediate_results <- function(extracted_data, batch_number) {
  saveRDS(extracted_data, paste0("extracted_data_batch_", batch_number, ".rds"))
}

# Function to process the DOIs and save results in batches
process_and_save_data <- function(dois, email) {
  # Fetch data in parallel
  fetched_data <- fetch_openalex_data_with_progress(dois, email)
  
  # Extract the relevant fields
  extracted_data <- lapply(fetched_data, extract_openalex_data)
  
  # Standardize column names
  column_names <- c("abstract", "so", "host_organization", "type", "is_retracted", 
                    "any_repository_has_fulltext", "language", "publication_year", 
                    "is_oa", "type_crossref", "countries_distinct_count", 
                    "institutions_distinct_count", "has_fulltext", 
                    "cited_by_count", "primary_topic_display_name", "referenced_works_count", 
                    "institution_country_code")
  
  # Ensure all data frames have the same columns
  extracted_data <- lapply(extracted_data, function(df) {
    missing_cols <- setdiff(column_names, names(df))
    df[missing_cols] <- NA
    df <- df[column_names]
    return(df)
  })
  
  # Combine all extracted data into a single data frame
  fetched_data_df <- do.call(rbind, extracted_data)
  
  # Save data in batches of 1000 records
  batch_size <- 1000
  num_batches <- ceiling(nrow(fetched_data_df) / batch_size)
  
  for (batch_number in 1:num_batches) {
    start_row <- (batch_number - 1) * batch_size + 1
    end_row <- min(batch_number * batch_size, nrow(fetched_data_df))
    batch_data <- fetched_data_df[start_row:end_row, ]
    
    # Save the batch to an RDS file
    save_intermediate_results(batch_data, batch_number)
  }
  
  return(fetched_data_df)
}

# Fetch and process data for the altmetrics dataset
dois <- altmetrics_data$DOI  # Replace with your actual dataset
email <- "a.watt2@lse.ac.uk"  # Replace with your email

final_data <- process_and_save_data(dois, email)

# Optionally, you can merge the processed data with the original dataset
merged_data <- cbind(altmetrics_data, final_data)

# Print merged data
print(merged_data)

```

```{r}
merged_data_clean <- merged_data %>% 
  select(`Title`,`abstract`,`DOI`,`publication_year`,`type`, `type_crossref`, `Journal/Collection Title`,`Publisher Names`,`host_organization`,`Funder`,`so`,`language`,`institution_country_code`,`countries_distinct_count`, `institutions_distinct_count`, `has_fulltext`, `is_oa`,`primary_topic_display_name`,`News mentions`,`Policy mentions`, `X mentions`,`Facebook mentions`,`Number of Mendeley readers`,`Altmetric Attention Score`,`cited_by_count`,`is_retracted`)

rownames(merged_data_clean) <- NULL
```

```{r}

```
